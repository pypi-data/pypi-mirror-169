Metadata-Version: 2.1
Name: gazeclassify
Version: 0.9.2
Summary: Algorithmic eye-tracking analysis
Home-page: https://github.com/footballdaniel/gazeclassify
Author: Daniel MÃ¼ller
Author-email: daniel@science.football
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: License :: OSI Approved
Classifier: Topic :: Scientific/Engineering
Requires-Python: >=3.6
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: tensorflow (>=2.5.0)
Requires-Dist: ffmpeg-python (>=0.2.0)
Requires-Dist: opencv-python (>=4.1.2)
Requires-Dist: pixellib (>=0.6.6)
Requires-Dist: tqdm (>=4.60.0)
Requires-Dist: moviepy (>=1.0.3)
Requires-Dist: tabulate (>=0.8.9)
Requires-Dist: pandas (>=1.2.5)

# GazeClassify
PiPy package to algorithmically annotate eye-tracking data. Recommended python version: 3.9

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/footballdaniel/gazeclassify/blob/main/colab.ipynb)
[![Test status](https://github.com/footballdaniel/gazeclassify/actions/workflows/test.yml/badge.svg)](https://github.com/footballdaniel/gazeclassify/actions/workflows/test.yml)
[![Downloads](https://pepy.tech/badge/gazeclassify)](https://pepy.tech/project/gazeclassify)
[![Downloads](https://pepy.tech/badge/gazeclassify/week)](https://pepy.tech/project/gazeclassify)

---
### What is GazeClassify?
 GazeClassify provides automatized and standardized eye-tracking annotation. Anyone can analyze gaze data online with less than 10 lines of code. 

![](https://raw.githubusercontent.com/footballdaniel/gazeclassify/main/gazeclassify/example_data/result_composite.jpg)
Exported `csv` will contain distance from gaze (red circle) to human joints (left image) and human shapes (right image) for each frame.

| frame number 	| classifier name 	| gaze_distance [pixel] 	| person_id 	| joint name 	|
|--------------	|-----------------	|-----------------------	|-----------	|------------	|
| 0            	| Human_Joints    	| 79                    	| 0         	| Neck       	|
| ...          	| ...             	| ...                   	| ...       	| ...        	|
| 0            	| Human_Shape     	| 0                     	| None      	| None       	|
| ...          	| ...             	| ...                   	| ...       	| ...        	|

### Run on example data

```python
from gazeclassify import Analysis, PupilLoader, SemanticSegmentation, InstanceSegmentation
from gazeclassify import example_trial

analysis = Analysis()

PupilLoader(analysis).from_recordings_folder(example_trial())

SemanticSegmentation(analysis).classify("Human_Shape")
InstanceSegmentation(analysis).classify("Human_Joints")

analysis.save_to_csv()
```

### Run on your own data
Capture eye tracking data from a Pupil eye tracker. Then, [export the data](https://docs.pupil-labs.com/core/#_8-export-data) using Pupil software. You will get a folder with the exported world video and the gaze timestamps. Finally, let gazeclassify analyze the exported data:

```python
from gazeclassify import Analysis, PupilLoader, SemanticSegmentation, InstanceSegmentation

analysis = Analysis()

PupilLoader(analysis).from_recordings_folder("path/to/your/folder_with_exported_data/")

SemanticSegmentation(analysis).classify("Human_Shape")
InstanceSegmentation(analysis).classify("Human_Joints")

analysis.save_to_csv()
```

### Citation
Please [cite this paper](https://dl.acm.org/doi/10.1145/3450341.3458886) in your publications if [GazeClassify](https://www.growkudos.com/publications/10.1145%25252F3450341.3458886/reader) helps your research. 

```
@inproceedings{10.1145/3450341.3458886,
  author = {M\"{u}ller, Daniel and Mann, David},
  title = {Algorithmic Gaze Classification for Mobile Eye-Tracking},
  year = {2021},
  booktitle = {ACM Symposium on Eye Tracking Research and Applications}
}
```

