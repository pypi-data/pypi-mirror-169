{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorchExample.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nERAUAWMhca4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchtext.datasets as td\n",
        "from pytorch_lightning.callbacks import (\n",
        "    EarlyStopping,\n",
        "    ModelCheckpoint,\n",
        "    LearningRateMonitor,\n",
        ")\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchdata.datapipes.iter import IterDataPipe\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from transformers import BertModel, BertTokenizer, AdamW\n",
        "\n",
        "\n",
        "import newron\n",
        "import newron.pytorch\n",
        "\n",
        "remote_server_uri = SERVER_URI # set to your server URI\n",
        "newron.set_tracking_uri(remote_server_uri)\n",
        "exp_name = \"pytorch_Example\"\n",
        "newron.set_experiment(exp_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_20newsgroups(num_samples):\n",
        "    categories = [\"alt.atheism\", \"talk.religion.misc\", \"comp.graphics\", \"sci.space\"]\n",
        "    X, y = fetch_20newsgroups(subset=\"train\", categories=categories, return_X_y=True)\n",
        "    return pd.DataFrame(data=X, columns=[\"description\"]).assign(label=y).sample(n=num_samples)\n",
        "\n",
        "\n",
        "def get_ag_news(num_samples):\n",
        "    # reading the input\n",
        "    td.AG_NEWS(root=\"data\", split=(\"train\", \"test\"))\n",
        "    train_csv_path = \"data/AG_NEWS/train.csv\"\n",
        "    return (\n",
        "        pd.read_csv(train_csv_path, usecols=[0, 2], names=[\"label\", \"description\"])\n",
        "        .assign(label=lambda df: df[\"label\"] - 1)  # make labels zero-based\n",
        "        .sample(n=num_samples)\n",
        "    )"
      ],
      "metadata": {
        "id": "CmeBbtk4a8Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NewsDataset(IterDataPipe):\n",
        "    def __init__(self, tokenizer, source, max_length, num_samples, dataset=\"20newsgroups\"):\n",
        "        \"\"\"\n",
        "        Custom Dataset - Converts the input text and label to tensor\n",
        "        :param tokenizer: bert tokenizer\n",
        "        :param source: data source - Either a dataframe or DataPipe\n",
        "        :param max_length: maximum length of the news text\n",
        "        :param num_samples: number of samples to load\n",
        "        :param dataset: Dataset type - 20newsgroups or ag_news\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.source = source\n",
        "        self.start = 0\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.dataset = dataset\n",
        "        self.end = num_samples\n",
        "\n",
        "    def __iter__(self):\n",
        "        worker_info = torch.utils.data.get_worker_info()\n",
        "        if worker_info is None:\n",
        "            iter_start = self.start\n",
        "            iter_end = self.end\n",
        "        else:\n",
        "            per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n",
        "            worker_id = worker_info.id\n",
        "            iter_start = self.start + worker_id * per_worker\n",
        "            iter_end = min(iter_start + per_worker, self.end)\n",
        "\n",
        "        for idx in range(iter_start, iter_end):\n",
        "            if self.dataset == \"20newsgroups\":\n",
        "                review = str(self.source[\"description\"].iloc[idx])\n",
        "                target = int(self.source[\"label\"].iloc[idx])\n",
        "            else:\n",
        "                target, review = self.source[idx]\n",
        "                target -= 1\n",
        "            encoding = self.tokenizer.encode_plus(\n",
        "                review,\n",
        "                add_special_tokens=True,\n",
        "                max_length=self.max_length,\n",
        "                return_token_type_ids=False,\n",
        "                padding=\"max_length\",\n",
        "                return_attention_mask=True,\n",
        "                return_tensors=\"pt\",\n",
        "                truncation=True,\n",
        "            )\n",
        "\n",
        "            yield {\n",
        "                \"review_text\": review,\n",
        "                \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
        "                \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
        "                \"targets\": torch.tensor(target, dtype=torch.long),\n",
        "            }\n"
      ],
      "metadata": {
        "id": "TtOy6YSsa8uD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Initialization of inherited lightning data module\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.PRE_TRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
        "        self.train_dataset = None\n",
        "        self.val_dataset = None\n",
        "        self.test_dataset = None\n",
        "        self.MAX_LEN = 100\n",
        "        self.encoding = None\n",
        "        self.tokenizer = None\n",
        "        self.args = kwargs\n",
        "        self.dataset = self.args[\"dataset\"]\n",
        "        self.train_count = None\n",
        "        self.val_count = None\n",
        "        self.test_count = None\n",
        "        self.RANDOM_SEED = 42\n",
        "        self.news_group_df = None\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"\n",
        "        Downloads the ag_news or 20newsgroup dataset and initializes bert tokenizer\n",
        "        \"\"\"\n",
        "        np.random.seed(self.RANDOM_SEED)\n",
        "        torch.manual_seed(self.RANDOM_SEED)\n",
        "\n",
        "        if self.dataset == \"20newsgroups\":\n",
        "            num_samples = self.args[\"num_samples\"]\n",
        "            self.news_group_df = (\n",
        "                get_20newsgroups(num_samples)\n",
        "                if self.args[\"dataset\"] == \"20newsgroups\"\n",
        "                else get_ag_news(num_samples)\n",
        "            )\n",
        "        else:\n",
        "            train_iter, test_iter = AG_NEWS()\n",
        "            self.train_dataset = to_map_style_dataset(train_iter)\n",
        "            self.test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(self.PRE_TRAINED_MODEL_NAME)\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"\n",
        "        Split the data into train, test, validation data\n",
        "        :param stage: Stage - training or testing\n",
        "        \"\"\"\n",
        "        if stage == \"fit\":\n",
        "            if self.dataset == \"20newsgroups\":\n",
        "                self.train_dataset, self.test_dataset = train_test_split(\n",
        "                    self.news_group_df,\n",
        "                    test_size=0.3,\n",
        "                    random_state=self.RANDOM_SEED,\n",
        "                    stratify=self.news_group_df[\"label\"],\n",
        "                )\n",
        "                self.val_dataset, self.test_dataset = train_test_split(\n",
        "                    self.test_dataset,\n",
        "                    test_size=0.5,\n",
        "                    random_state=self.RANDOM_SEED,\n",
        "                    stratify=self.test_dataset[\"label\"],\n",
        "                )\n",
        "\n",
        "                self.train_count = len(self.train_dataset)\n",
        "                self.val_count = len(self.val_dataset)\n",
        "                self.test_count = len(self.test_dataset)\n",
        "            else:\n",
        "\n",
        "                num_train = int(len(self.train_dataset) * 0.95)\n",
        "                self.train_dataset, self.val_dataset = random_split(\n",
        "                    self.train_dataset, [num_train, len(self.train_dataset) - num_train]\n",
        "                )\n",
        "\n",
        "                self.train_count = self.args.get(\"num_samples\")\n",
        "                self.val_count = int(self.train_count / 10)\n",
        "                self.test_count = int(self.train_count / 10)\n",
        "                self.train_count = self.train_count - (self.val_count + self.test_count)\n",
        "\n",
        "            print(\"Number of samples used for training: {}\".format(self.train_count))\n",
        "            print(\"Number of samples used for validation: {}\".format(self.val_count))\n",
        "            print(\"Number of samples used for test: {}\".format(self.test_count))\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        \"\"\"\n",
        "        Returns the review text and the targets of the specified item\n",
        "        :param parent_parser: Application specific parser\n",
        "        :return: Returns the augmented argument parser\n",
        "        \"\"\"\n",
        "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
        "        parser.add_argument(\n",
        "            \"--batch_size\",\n",
        "            type=int,\n",
        "            default=16,\n",
        "            metavar=\"N\",\n",
        "            help=\"input batch size for training (default: 16)\",\n",
        "        )\n",
        "        parser.add_argument(\n",
        "            \"--num_workers\",\n",
        "            type=int,\n",
        "            default=3,\n",
        "            metavar=\"N\",\n",
        "            help=\"number of workers (default: 3)\",\n",
        "        )\n",
        "        return parser\n",
        "\n",
        "    def create_data_loader(self, source, count):\n",
        "        \"\"\"\n",
        "        Generic data loader function\n",
        "        :param df: Input dataframe\n",
        "        :param tokenizer: bert tokenizer\n",
        "        :return: Returns the constructed dataloader\n",
        "        \"\"\"\n",
        "        ds = NewsDataset(\n",
        "            source=source,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_length=self.MAX_LEN,\n",
        "            num_samples=count,\n",
        "            dataset=self.dataset,\n",
        "        )\n",
        "\n",
        "        return DataLoader(\n",
        "            ds, batch_size=self.args[\"batch_size\"], num_workers=self.args[\"num_workers\"]\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"\n",
        "        :return: output - Train data loader for the given input\n",
        "        \"\"\"\n",
        "        return self.create_data_loader(source=self.train_dataset, count=self.train_count)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\"\n",
        "        :return: output - Validation data loader for the given input\n",
        "        \"\"\"\n",
        "        return self.create_data_loader(source=self.val_dataset, count=self.val_count)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        \"\"\"\n",
        "        :return: output - Test data loader for the given input\n",
        "        \"\"\"\n",
        "        return self.create_data_loader(source=self.test_dataset, count=self.test_count)"
      ],
      "metadata": {
        "id": "MAMxQMzQa_mE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertNewsClassifier(pl.LightningModule):\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the network, optimizer and scheduler\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.PRE_TRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
        "        self.bert_model = BertModel.from_pretrained(self.PRE_TRAINED_MODEL_NAME)\n",
        "        for param in self.bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.drop = nn.Dropout(p=0.2)\n",
        "        # assigning labels\n",
        "        self.class_names = (\n",
        "            [\"alt.atheism\", \"talk.religion.misc\", \"comp.graphics\", \"sci.space\"]\n",
        "            if kwargs[\"dataset\"] == \"20newsgroups\"\n",
        "            else [\"world\", \"Sports\", \"Business\", \"Sci/Tech\"]\n",
        "        )\n",
        "        n_classes = len(self.class_names)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.bert_model.config.hidden_size, 512)\n",
        "        self.out = nn.Linear(512, n_classes)\n",
        "\n",
        "        self.scheduler = None\n",
        "        self.optimizer = None\n",
        "        self.args = kwargs\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        :param input_ids: Input data\n",
        "        :param attention_maks: Attention mask value\n",
        "        :return: output - Type of news for the given news snippet\n",
        "        \"\"\"\n",
        "        output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        output = F.relu(self.fc1(output.pooler_output))\n",
        "        output = self.drop(output)\n",
        "        output = self.out(output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser):\n",
        "        \"\"\"\n",
        "        Returns the review text and the targets of the specified item\n",
        "        :param parent_parser: Application specific parser\n",
        "        :return: Returns the augmented argument parser\n",
        "        \"\"\"\n",
        "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
        "        parser.add_argument(\n",
        "            \"--lr\",\n",
        "            type=float,\n",
        "            default=0.001,\n",
        "            metavar=\"LR\",\n",
        "            help=\"learning rate (default: 0.001)\",\n",
        "        )\n",
        "        return parser\n",
        "\n",
        "    def training_step(self, train_batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Training the data as batches and returns training loss on each batch\n",
        "        :param train_batch Batch data\n",
        "        :param batch_idx: Batch indices\n",
        "        :return: output - Training loss\n",
        "        \"\"\"\n",
        "        input_ids = train_batch[\"input_ids\"].to(self.device)\n",
        "        attention_mask = train_batch[\"attention_mask\"].to(self.device)\n",
        "        targets = train_batch[\"targets\"].to(self.device)\n",
        "        output = self.forward(input_ids, attention_mask)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def test_step(self, test_batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Performs test and computes the accuracy of the model\n",
        "        :param test_batch: Batch data\n",
        "        :param batch_idx: Batch indices\n",
        "        :return: output - Testing accuracy\n",
        "        \"\"\"\n",
        "        input_ids = test_batch[\"input_ids\"].to(self.device)\n",
        "        attention_mask = test_batch[\"attention_mask\"].to(self.device)\n",
        "        targets = test_batch[\"targets\"].to(self.device)\n",
        "        output = self.forward(input_ids, attention_mask)\n",
        "        _, y_hat = torch.max(output, dim=1)\n",
        "        test_acc = accuracy_score(y_hat.cpu(), targets.cpu())\n",
        "        return {\"test_acc\": torch.tensor(test_acc)}\n",
        "\n",
        "    def validation_step(self, val_batch, batch_idx):\n",
        "        \"\"\"\n",
        "        Performs validation of data in batches\n",
        "        :param val_batch: Batch data\n",
        "        :param batch_idx: Batch indices\n",
        "        :return: output - valid step loss\n",
        "        \"\"\"\n",
        "\n",
        "        input_ids = val_batch[\"input_ids\"].to(self.device)\n",
        "        attention_mask = val_batch[\"attention_mask\"].to(self.device)\n",
        "        targets = val_batch[\"targets\"].to(self.device)\n",
        "        output = self.forward(input_ids, attention_mask)\n",
        "        loss = F.cross_entropy(output, targets)\n",
        "        return {\"val_step_loss\": loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        \"\"\"\n",
        "        Computes average validation accuracy\n",
        "        :param outputs: outputs after every epoch end\n",
        "        :return: output - average valid loss\n",
        "        \"\"\"\n",
        "        avg_loss = torch.stack([x[\"val_step_loss\"] for x in outputs]).mean()\n",
        "        self.log(\"val_loss\", avg_loss, sync_dist=True)\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        \"\"\"\n",
        "        Computes average test accuracy score\n",
        "        :param outputs: outputs after every epoch end\n",
        "        :return: output - average test loss\n",
        "        \"\"\"\n",
        "        avg_test_acc = torch.stack([x[\"test_acc\"] for x in outputs]).mean()\n",
        "        self.log(\"avg_test_acc\", avg_test_acc)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        Initializes the optimizer and learning rate scheduler\n",
        "        :return: output - Initialized optimizer and scheduler\n",
        "        \"\"\"\n",
        "        self.optimizer = AdamW(self.parameters(), lr=self.args[\"lr\"])\n",
        "        self.scheduler = {\n",
        "            \"scheduler\": torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                self.optimizer,\n",
        "                mode=\"min\",\n",
        "                factor=0.2,\n",
        "                patience=2,\n",
        "                min_lr=1e-6,\n",
        "                verbose=True,\n",
        "            ),\n",
        "            \"monitor\": \"val_loss\",\n",
        "        }\n",
        "        return [self.optimizer], [self.scheduler]\n"
      ],
      "metadata": {
        "id": "YhK1B1ehbFAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 2000\n",
        "dataset= \"20newsgroups\"\n",
        "\n",
        "dm = BertDataModule()\n",
        "dm.prepare_data()\n",
        "\n",
        "model = BertNewsClassifier()\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=True)\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    dirpath=os.getcwd(),\n",
        "    save_top_k=1,\n",
        "    verbose=True,\n",
        "    monitor=\"val_loss\",\n",
        "    mode=\"min\",\n",
        ")\n",
        "\n",
        "lr_logger = LearningRateMonitor()\n",
        "\n",
        "trainer = pl.Trainer.from_argparse_args(\n",
        "    args,\n",
        "    callbacks=[lr_logger, early_stopping, checkpoint_callback],\n",
        "    enable_checkpointing=True,\n",
        ")\n",
        "\n",
        "newron.pytorch.autolog()\n",
        "\n",
        "trainer.fit(model, dm)\n",
        "trainer.test(model, datamodule=dm)"
      ],
      "metadata": {
        "id": "WvzlRsO5Zxaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Q7eea323OeK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xCAWGOUAOasl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7e_xyOxnO3kN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}