{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [STF-IDF](https://arxiv.org/abs/2209.14281): Multilingual Search with Subword TF-IDF\n",
        "\n",
        "Multilingual search can be achieved with subword tokenization. The accuracy of traditional TF-IDF approaches depend on manually curated tokenization, stop words and stemming rules, whereas subword TF-IDF (STF-IDF) can offer higher accuracy without such heuristics. Moreover, multilingual support can be incorporated inherently as part of the subword tokenization model training. XQuAD evaluation demonstrates the advantages of STF-IDF: superior information retrieval accuracy of 85.4% for English and over 80% for 10 other languages without any heuristics-based preprocessing. The software to reproduce these results are open-sourced as a part of [Text2Text](https://github.com/artitw/text2text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "@article{stfidf,\n",
        "  doi = {10.48550/ARXIV.2209.14281},\n",
        "  url = {https://arxiv.org/abs/2209.14281},\n",
        "  author = {Wangperawong, Artit},\n",
        "  title = {Multilingual Search with Subword TF-IDF},\n",
        "  publisher = {arXiv},\n",
        "  year = {2022},\n",
        "}\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rd2Sp0tbOXvB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQOmVASAEuxF",
        "outputId": "c3b792ce-ba27-4bb6-d923-f3e6c8e36860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selecting previously unselected package libomp5:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 159447 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\r\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\r\n",
            "Selecting previously unselected package libomp-dev.\r\n",
            "Preparing to unpack .../libomp-dev_5.0.1-1_amd64.deb ...\r\n",
            "Unpacking libomp-dev (5.0.1-1) ...\r\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\r\n",
            "Setting up libomp-dev (5.0.1-1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.5) ...\r\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -q -U text2text\n",
        "sudo apt-get -qq install libopenblas-dev\n",
        "sudo apt-get -qq install libomp-dev"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_data(lang_code=\"en\"):\n",
        "  url = f\"https://raw.githubusercontent.com/deepmind/xquad/master/xquad.{lang_code}.json\"\n",
        "  r = requests.get(url)\n",
        "  d = r.json()\n",
        "  corpus = []\n",
        "  queries = []\n",
        "  id = 0\n",
        "  for a in d[\"data\"]:\n",
        "    for p in a[\"paragraphs\"]:\n",
        "      c = p[\"context\"]\n",
        "      corpus.append((id,c))\n",
        "      for qa in p[\"qas\"]:\n",
        "        q = qa[\"question\"]\n",
        "        queries.append((id,q))\n",
        "      id += 1\n",
        "  cids, c = zip(*corpus)\n",
        "  qids, q = zip(*queries)\n",
        "  return cids, c, qids, q"
      ],
      "metadata": {
        "id": "MnyW5sdNFTgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import text2text as t2t\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_stfidf(corpus_ids, corpus, ans_ids, queries):\n",
        "  index = t2t.Handler(corpus).index(ids=corpus_ids)\n",
        "  dist, pred_ids = index.search(queries, k=1)\n",
        "  accuracy = np.sum(pred_ids.reshape(-1)==np.array(ans_ids))/len(ans_ids)\n",
        "  return accuracy"
      ],
      "metadata": {
        "id": "D_deILjSGfq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang_codes = [\"en\",\"es\",\"de\",\"el\",\"ru\",\"tr\",\"ar\",\"vi\",\"th\",\"zh\",\"hi\",\"ro\"]\n",
        "\n",
        "for lang in lang_codes:\n",
        "  corpus_ids, corpus, ans_ids, queries = get_data(lang_code=lang)\n",
        "  acc = evaluate_stfidf(corpus_ids, corpus, ans_ids, queries)\n",
        "  print(lang, acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duu5XqIIq3cm",
        "outputId": "6755ed62-a9d6-4504-9ec6-3c060ee9d86b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "en 0.853781512605042\n",
            "es 0.8579831932773109\n",
            "de 0.8487394957983193\n",
            "el 0.8134453781512605\n",
            "ru 0.8294117647058824\n",
            "tr 0.8008403361344538\n",
            "ar 0.7705882352941177\n",
            "vi 0.8445378151260504\n",
            "th 0.8352941176470589\n",
            "zh 0.8243697478991596\n",
            "hi 0.8092436974789916\n",
            "ro 0.8504201680672269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ctypes import c_bool\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def _tokenizer(strategy, s):\n",
        "  s = nltk.word_tokenize(s)\n",
        "  if strategy == \"word\":\n",
        "    return s\n",
        "\n",
        "  if strategy == \"word>stem\":\n",
        "    return [stemmer.stem(item) for item in s]\n",
        "\n",
        "  s = [word for word in s if word not in ENGLISH_STOP_WORDS]\n",
        "  if strategy == \"word>stop\":\n",
        "    return s\n",
        "    \n",
        "  s = [stemmer.stem(item) for item in s]\n",
        "  if strategy == \"word>stop>stem\":\n",
        "    return s\n",
        "  return s\n",
        "\n",
        "corpus_ids, corpus, ans_ids, queries = get_data(lang_code=\"en\")\n",
        "\n",
        "for strat in [\"word\", \"word>stop\", \"word>stem\", \"word>stop>stem\"]:\n",
        "  vectorizer = TfidfVectorizer(tokenizer=lambda x: _tokenizer(strat, x))\n",
        "  C = vectorizer.fit_transform(corpus)\n",
        "  Q = vectorizer.transform(queries)\n",
        "  scores = np.matmul(C.toarray(), Q.transpose().toarray())\n",
        "  pred_ids = np.argmax(scores, axis=0)\n",
        "  accuracy = np.sum(pred_ids==np.array(ans_ids))/len(ans_ids)\n",
        "  print(strat, accuracy)\n",
        "  c_subword = [\" \".join(_tokenizer(strat, d)) for d in corpus]\n",
        "  q_subword = [\" \".join(_tokenizer(strat, q)) for q in queries]\n",
        "  acc = evaluate_stfidf(corpus_ids, c_subword, ans_ids, q_subword)\n",
        "  print(strat+\">subword\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7ze6icqWwae",
        "outputId": "c011cfb3-ad58-4ec1-f2fd-96071c6d1ff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word 0.8420168067226891\n",
            "word>subword 0.8495798319327731\n",
            "word>stop 0.8394957983193277\n",
            "word>stop>subword 0.8420168067226891\n",
            "word>stem 0.8487394957983193\n",
            "word>stem>subword 0.853781512605042\n",
            "word>stop>stem 0.8521008403361344\n",
            "word>stop>stem>subword 0.8445378151260504\n"
          ]
        }
      ]
    }
  ]
}